{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5x7InUf8JcG7zuXpdoQrX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiujiaZ/restless_bandit_basics/blob/main/Whittle_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A toy examples of whittle index."
      ],
      "metadata": {
        "id": "ujpkNNkJiWrG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VQcIVC6l6VTX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verify approximated WI with exact WI with a toy example from :\n",
        "#   https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8945748\n",
        "\n",
        "# p(a = 1, s, s')\n",
        "p_1 = np.array([\n",
        "    [.5, .5, 0, 0],\n",
        "    [0, .5, .5, 0],\n",
        "    [0, 0, .5, .5],\n",
        "    [.5, 0, 0, .5],])\n",
        "\n",
        "# p(a = 0, s, s')\n",
        "p_0 = np.array([\n",
        "    [.5, 0, 0, .5],\n",
        "    [.5, .5, 0, 0],\n",
        "    [0, .5, .5, 0],\n",
        "    [0, 0, .5, .5],])\n",
        "\n",
        "# p(a, s, s'):\n",
        "transitions = np.array([p_0, p_1])\n",
        "\n",
        "# reward vector corresponding to each states, (same for all actions)\n",
        "R = np.array([-1, 0, 0, 1])\n",
        "\n",
        "# exact WI:\n",
        "WI_exact = [-.5, .5, 1, -1]\n",
        "\n",
        "n_actions, n_states = transitions.shape[:-1]"
      ],
      "metadata": {
        "id": "IMrIk89c-ooC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(transitions, R, lamb_val, gamma, epsilon=1e-2):\n",
        "\n",
        "    \"\"\"\n",
        "    value iteration for a MDS:\n",
        "\n",
        "        @param transitions: transition matrix p[a, s, s']\n",
        "        @param R:           reward for all states R[s]\n",
        "        @param lamb_val:    agrangian multiplier associated with formulation of WI\n",
        "        @param gamma:       discounted factor for reward\n",
        "        @param epsilon:     tolerance to terminate value iteration\n",
        "\n",
        "        @return Q_func:     Q_values for each state and actions [s, a]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n_actions, n_states = transitions.shape[:-1]\n",
        "    value_func = np.random.rand(n_states)\n",
        "    delta = np.ones((n_states))\n",
        "    iters = 0\n",
        "\n",
        "    while np.max(delta) >= epsilon:\n",
        "        iters += 1\n",
        "        orig_value_func = np.copy(value_func)\n",
        "\n",
        "        Q_func = np.zeros((n_states, n_actions))\n",
        "        for s in range(n_states):\n",
        "\n",
        "            for a in range(n_actions):\n",
        "                Q_func[s, a] += - a * lamb_val + R[s] + gamma * np.dot(transitions[a, s, :], value_func)\n",
        "            value_func[s] = np.max(Q_func[s, :])\n",
        "\n",
        "        delta = np.abs(orig_value_func - value_func)\n",
        "\n",
        "    return Q_func\n",
        "\n",
        "\n",
        "def whittle_index(transitions, state, R, gamma, lb, ub, subsidy_break, epsilon=1e-4):\n",
        "    \"\"\"\n",
        "    whittle index for a specified state using binary search: https://arxiv.org/pdf/2205.15372.pdf\n",
        "\n",
        "        @param transitions:     transition matrix p[a, s, s']\n",
        "        @param state:           a single specified state \\in [S]\n",
        "        @param R:               reward for all states R[s]\n",
        "        @param lamb_val:        lgrangian multiplier associated with formulation of WI\n",
        "        @param gamma:           discounted factor for reward\n",
        "        @param lb, ub:          initial lower / upper bound of WI\n",
        "        @param subsidy_break:   lower tolerance of WI (if returned, decrease lb)\n",
        "        @param epsilon:         tolerance to terminate binary search\n",
        "\n",
        "        @return subsidy:        approximated whittle index for specified state\n",
        "    \"\"\"\n",
        "\n",
        "    while abs(ub - lb) > epsilon:\n",
        "        lamb_val = (lb + ub) / 2\n",
        "        # print('lamb', lamb_val, lb, ub)\n",
        "\n",
        "        # need to adjust initial lb\n",
        "        if ub < subsidy_break:\n",
        "            # print('breaking early!', subsidy_break, lb, ub)\n",
        "            return -10\n",
        "\n",
        "        Q_func = value_iteration(transitions, R, lamb_val, gamma)\n",
        "\n",
        "        # binary search:\n",
        "        action = np.argmax(Q_func[state, :])\n",
        "\n",
        "        # Q(s, 0) > Q(s, 1)_{lamb_val}: lamb_val in smaller interval\n",
        "        if action == 0:\n",
        "            ub = lamb_val\n",
        "        # Q(s, 0) < Q(s, 1)_{lamb_val}: lamb_val in bigger interval\n",
        "        elif action == 1:\n",
        "            lb = lamb_val\n",
        "        else:\n",
        "            raise Error(f'action not binary: {action}')\n",
        "\n",
        "    subsidy = (ub + lb) / 2\n",
        "    return subsidy"
      ],
      "metadata": {
        "id": "OwCsA2Y-6cg6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check against exact WI"
      ],
      "metadata": {
        "id": "w0F48hBFioJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check with exact WI:\n",
        "WI_approx = list()\n",
        "for state in range(4):\n",
        "    WI_approx.append(whittle_index(transitions, state, R, gamma = 0.99, lb = -1, ub = 1, subsidy_break=-1))\n",
        "print(f'exact WI {WI_exact}, approximated WI {WI_approx}')\n",
        "\n",
        "# this means we perfer to pull arm in state 2, 1, 0, 3, respectively.\n",
        "# eg: assume we have 4 arms, each in distinct state.\n",
        "#     for budget constraint of 2, we will pull arm in state 2 and in state 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVShJmhzDRAS",
        "outputId": "a16bab20-b585-4d7b-9bd7-9fbfad2b0e5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exact WI [-0.5, 0.5, 1, -1], approximated WI [-0.497467041015625, 0.492523193359375, 0.997589111328125, -0.992401123046875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check against random policy"
      ],
      "metadata": {
        "id": "U96EOvGOiwvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N arms, each reprented by a MDS, one step simulation\n",
        "def Arms_step(init_states, actions, transitions):\n",
        "    \"\"\"\n",
        "        @param initial_states: vector with one entry 1\n",
        "        @param actions:        vectors with arms should be pulled as 1, otherwise 0\n",
        "        @param transitions:    transition matrix p[a, s, s']\n",
        "\n",
        "        @param current_states: one_step randomized state based on p[s'| a ]\n",
        "    \"\"\"\n",
        "\n",
        "    n_actions, n_states = transitions.shape[:-1]\n",
        "    current_states = np.zeros_like(init_states)\n",
        "    reward = 0\n",
        "\n",
        "    for i, (s, a) in enumerate(zip(init_states, actions)):\n",
        "        states = np.zeros((1, n_states))\n",
        "        states[0,s] = 1\n",
        "\n",
        "        pos_state = (states @ transitions[a]).reshape(-1)\n",
        "\n",
        "        # sample through p[s'|a]\n",
        "        current_states[i] = np.random.choice(n_states, size=1, p = pos_state)\n",
        "\n",
        "    # compute reward from pulled arms:\n",
        "    reward += (R[current_states] * actions).sum()\n",
        "\n",
        "    return current_states, reward\n",
        "\n"
      ],
      "metadata": {
        "id": "G3I00zY5TmXq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_states = np.array([2, 0, 1, 3])\n",
        "\n",
        "# WI policy tells us to pull arm 0 and 3:\n",
        "WI_actions = np.array([1, 0, 1, 0])\n",
        "WI_currant_state, WI_reward = Arms_step(init_states, WI_actions, transitions)\n",
        "\n",
        "# compare with random selection:\n",
        "indx = np.random.choice(n_states, size=2, replace = False)\n",
        "random_actions = np.zeros_like(WI_actions)\n",
        "random_actions[indx] = 1\n",
        "random_currant_state, random_reward = Arms_step(init_states, random_actions, transitions)\n",
        "\n",
        "print(f'WI actions {WI_actions}, random actions {random_actions}')\n",
        "print(f'WI reward {WI_reward}, random reward {random_reward}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG3a9CSnVnAy",
        "outputId": "180a8d75-7060-4dd7-a6e8-354aedf7a078"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WI actions [1 0 1 0], random actions [1 0 1 0]\n",
            "WI reward 1, random reward 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple simulations, average reward\n",
        "T = 1000\n",
        "\n",
        "WI_reward = 0\n",
        "random_reward = 0\n",
        "\n",
        "for t in range(T):\n",
        "    _, reward = Arms_step(init_states, WI_actions, transitions)\n",
        "    WI_reward += reward\n",
        "\n",
        "    # compare with random selection:\n",
        "    indx = np.random.choice(n_states, size=2, replace = False)\n",
        "    random_actions = np.zeros_like(WI_actions)\n",
        "    random_actions[indx] = 1\n",
        "    _, reward = Arms_step(init_states, random_actions, transitions)\n",
        "    random_reward += reward\n",
        "\n",
        "WI_reward /= 1000\n",
        "random_reward /= 1000\n",
        "\n",
        "print(f'WI reward {WI_reward}, random reward {random_reward}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4x2OON7fgpZ",
        "outputId": "a7e39312-0808-4b56-8169-4912aecd23df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WI reward 0.498, random reward 0.012\n"
          ]
        }
      ]
    }
  ]
}